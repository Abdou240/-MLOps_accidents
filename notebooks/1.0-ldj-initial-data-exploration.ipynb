{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import ensemble\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm, model_selection\n",
    "from sklearn import neighbors\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Abderrahmen Mansour\\\\Documents\\\\Project\\\\MLOps_Accidents\\\\notebooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(Path().absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Abderrahmen Mansour\\\\Documents\\\\Project\\\\MLOps_Accidents\\\\notebooks../data/raw/usagers-2021.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#--Importing dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Change it to your absolut path\u001b[39;00m\n\u001b[0;32m      4\u001b[0m abs_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(Path()\u001b[38;5;241m.\u001b[39mabsolute())\n\u001b[1;32m----> 6\u001b[0m df_users\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabs_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/raw/usagers-2021.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m df_places\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(abs_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/raw/lieux-2021.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m df_caract\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(abs_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/raw/caracteristiques-2021.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    663\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    664\u001b[0m     dialect,\n\u001b[0;32m    665\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    674\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    675\u001b[0m )\n\u001b[0;32m    676\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1212\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Abderrahmen Mansour\\\\Documents\\\\Project\\\\MLOps_Accidents\\\\notebooks../data/raw/usagers-2021.csv'"
     ]
    }
   ],
   "source": [
    "#--Importing dataset\n",
    "\n",
    "# Change it to your absolut path\n",
    "abs_path = str(Path().absolute())\n",
    "\n",
    "df_users=pd.read_csv(abs_path+\"/data/raw/usagers-2021.csv\", sep=\";\")\n",
    "df_places=pd.read_csv(abs_path+\"/data/raw/lieux-2021.csv\", sep=\";\", header=0, low_memory=False)\n",
    "df_caract=pd.read_csv(abs_path+\"/data/raw/caracteristiques-2021.csv\", sep = \";\", encoding='utf-8')\n",
    "df_veh=pd.read_csv(abs_path + \"/data/raw/vehicules-2021.csv\", sep=\";\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet Users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Visualizing dataset users \n",
    "\n",
    "df_users.head(15)\n",
    "df_users['grav'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Calculating the number of victims by accident \n",
    "\n",
    "nb_victim = pd.crosstab(df_users.Num_Acc, \"count\").reset_index()\n",
    "\n",
    "print(nb_victim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Calculating the number of vehicles involved in the accident \n",
    "nb_vehicules = pd.crosstab(df_veh.Num_Acc, \"count\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Changing the number associated to injured to make more sens\n",
    "\n",
    "df_users.grav.replace([1,2,3,4], [1,3,4,2], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Removing variables secu2 and secu3 because they don't bring anything relevant \n",
    "df_users.drop([\"secu2\", \"secu3\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Adding the age of the victims at the time of the accident\n",
    "\n",
    "#-Extracting the year of the accident thanks to the accident number\n",
    "df_users[\"year_acc\"] = df_users[\"Num_Acc\"].astype(str).apply(lambda x : x[:4]).astype(int)\n",
    "\n",
    "#-Calculating the age the victim\n",
    "df_users[\"victim_age\"] = df_users[\"year_acc\"]-df_users[\"an_nais\"]\n",
    "for i in df_users[\"victim_age\"] :\n",
    "  #- Replacing outliers by NaN\n",
    "  if (i>120)|(i<0):\n",
    "    df_users[\"victim_age\"].replace(i,np.nan)\n",
    "\n",
    "df_users.drop([\"year_acc\",\"an_nais\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Caracteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Replacing column named agg by agg_ since agg is a Python method\n",
    "\n",
    "df_caract.rename({\"agg\" : \"agg_\"},  inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Replacing Corsica code 2A and 2B by 201 and 202 \n",
    "corse_replace = {\"2A\":\"201\", \"2B\":\"202\"}\n",
    "df_caract[\"dep\"] = df_caract[\"dep\"].str.replace(\"2A\", \"201\")\n",
    "df_caract[\"dep\"] = df_caract[\"dep\"].str.replace(\"2B\", \"202\")\n",
    "df_caract[\"com\"] = df_caract[\"com\"].str.replace(\"2A\", \"201\")\n",
    "df_caract[\"com\"] = df_caract[\"com\"].str.replace(\"2B\", \"202\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Creating a column datetime\n",
    "df_caract['datetime_str'] = df_caract['jour'].astype(str) + '/' + df_caract['mois'].astype(str) + '/' + df_caract['an'].astype(str) + ' ' + df_caract['hrmn']\n",
    "df_caract['datetime'] = pd.to_datetime(df_caract['datetime_str'], format='%d/%m/%Y %H:%M')\n",
    "df_caract.drop(columns=['datetime_str'], inplace=True)\n",
    "df_caract[\"datetime\"].sort_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Creating a column hour that will replace hrmn\n",
    "\n",
    "df_caract[\"hour\"] = df_caract[\"hrmn\"].astype(str).apply(lambda x : x[:-3])\n",
    "df_caract.drop(['hrmn'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Convertir columns dep, com and hour into type int\n",
    "df_caract[[\"dep\",\"com\", \"hour\"]] = df_caract[[\"dep\",\"com\", \"hour\"]].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Converting columns lat and long into float type\n",
    "\n",
    "dico_to_float = { 'lat': float, 'long':float}\n",
    "\n",
    "df_caract[\"lat\"] = df_caract[\"lat\"].str.replace(',', '.')\n",
    "df_caract[\"long\"] = df_caract[\"long\"].str.replace(',', '.')\n",
    "df_caract = df_caract.astype(dico_to_float)\n",
    "\n",
    "df_caract.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Removing variable adr because not usable\n",
    "\n",
    "df_caract = df_caract.drop(columns = 'adr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caract.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Grouping the modalities of the atm ( Atmosheric Conditions) variable into 1 : Risky and 0 : Normal. We include Other in Normal.\n",
    "print(\"Modalities of the variable atm : \", df_caract['atm'].unique())\n",
    "dico = {1:0, 2:1, 3:1, 4:1, 5:1, 6:1,7:1, 8:0, 9:0}\n",
    "df_caract[\"atm\"] = df_caract[\"atm\"].replace(dico)\n",
    "df_caract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Visualizing correlations\n",
    "df_numerical = df_caract.select_dtypes(include=['int64', 'float64', 'datetime'])\n",
    "\n",
    "correlation_matrix = df_numerical.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Vehicles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_veh.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_veh.catv.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_veh.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Merging datasets users and vehicles\n",
    "\n",
    "fusion1= df_users.merge(df_veh, on = [\"Num_Acc\",\"num_veh\", \"id_vehicule\"], how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Keeping 1 line by accident and keeping the most severe injured person\n",
    "\n",
    "fusion1 = fusion1.sort_values(by = \"grav\", ascending = False)\n",
    "fusion1 = fusion1.drop_duplicates(subset = ['Num_Acc'], keep=\"first\")\n",
    "fusion1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion2 = fusion1.merge(df_places, on = \"Num_Acc\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fusion2.merge(df_caract, on = 'Num_Acc', how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Adding the number of victims\n",
    "df = df.merge(nb_victim, on = \"Num_Acc\", how = \"inner\")\n",
    "\n",
    "df.rename({\"count\" :\"nb_victim\"},axis = 1, inplace = True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Adding the number of vehicles \n",
    "\n",
    "df = df.merge(nb_vehicules, on = \"Num_Acc\", how = \"inner\") \n",
    "df.rename({\"count\" :\"nb_vehicules\"},axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['grav'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Modification of the target variable  : 1 : prioritary// 0 : non-prioritary\n",
    "\n",
    "df['grav'].replace([2,3,4], [0,1,1], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#--Calculating the number of missing values for each column\n",
    "missing_values_count = df.isnull().sum()\n",
    "\n",
    "#--Calculating the percentage of missing values for each variables \n",
    "total_cells = len(df)\n",
    "missing_percentage = (missing_values_count / total_cells) * 100\n",
    "\n",
    "#--Creating a new dataframe with the percentage of missing values \n",
    "missing_df = pd.DataFrame({'Column': missing_percentage.index, 'MissingPercentage': missing_percentage.values})\n",
    "\n",
    "missing_df = missing_df.sort_values(by='MissingPercentage', ascending=False)\n",
    "\n",
    "print(missing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Removing variables with more than 70% of missing values \n",
    "list_to_drop = ['v1', 'lartpc','occutc','v2','vosp','locp','etatp', 'infra', 'obs']\n",
    "\n",
    "df.drop(list_to_drop, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catv_value = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,30,31,32,33,34,35,36,37,38,39,40,41,42,43,50,60,80,99]\n",
    "catv_value_new = [0,1,1,2,1,1,6,2,5,5,5,5,5,4,4,4,4,4,3,3,4,4,1,1,1,1,1,6,6,3,3,3,3,1,1,1,1,1,0,0]\n",
    "df['catv'].replace(catv_value, catv_value_new, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Replacing values -1 et 0 par NaN\n",
    "col_to_replace0_na = [\"actp\", \"trajet\", \"catv\", \"motor\"]\n",
    "col_to_replace1_na = [\"actp\", \"trajet\", \"secu1\", \"catv\", \"obsm\", \"motor\", \"circ\", \"larrout\", \"surf\", \"situ\", \"vma\", \"atm\", \"col\"]\n",
    "df2 = df.copy()\n",
    "df[col_to_replace1_na] = df[col_to_replace1_na].replace(-1, np.nan)\n",
    "df[col_to_replace0_na] = df[col_to_replace0_na].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns[15:]:\n",
    "    print(f\"Modalities of the variable {column} : \", df[column].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Removing variables that do not match with our goal to predict the severity of the accident \n",
    "\n",
    "list_to_drop = ['senc', 'manv', 'choc', 'nbv', 'prof', 'plan', 'Num_Acc', 'id_vehicule', 'num_veh', 'pr', 'pr1', 'trajet' ]\n",
    "# ,'voie'\n",
    "df.drop(list_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Replacing modalities A and B by 10 and 11\n",
    "df[\"actp\"] = df[\"actp\"].str.replace(\"A\",\"10\")\n",
    "df[\"actp\"] = df[\"actp\"].str.replace(\"B\",\"11\")\n",
    "df[\"actp\"] = df[\"actp\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['voie'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Converting larrout into float\n",
    "df[\"larrout\"] = df[\"larrout\"].str.replace(\",\",\".\")\n",
    "df[\"larrout\"] = df[\"larrout\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Fill NaN with mode \n",
    "col_to_fill_na = [\"surf\", \"situ\", \"circ\", \"col\", \"motor\"]\n",
    "\n",
    "df[col_to_fill_na] = df[col_to_fill_na].fillna(df[col_to_fill_na].mode().iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop NaN \n",
    "\n",
    "df = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finale version of the preprocessing steps :\n",
    "\n",
    "#--Importing dataset\n",
    "df_users=pd.read_csv(abs_path+\"/data/raw/usagers-2021.csv\", sep=\";\")\n",
    "df_places=pd.read_csv(abs_path+\"/data/raw/lieux-2021.csv\", sep=\";\", header=0, low_memory=False)\n",
    "df_caract=pd.read_csv(abs_path+\"/data/raw/caracteristiques-2021.csv\", sep = \";\", encoding='utf-8')\n",
    "df_veh=pd.read_csv(abs_path+\"/data/raw/vehicules-2021.csv\", sep=\";\")\n",
    "\n",
    "\n",
    "nb_victim = pd.crosstab(df_users.Num_Acc, \"count\").reset_index()\n",
    "nb_vehicules = pd.crosstab(df_veh.Num_Acc, \"count\").reset_index()\n",
    "df_users[\"year_acc\"] = df_users[\"Num_Acc\"].astype(str).apply(lambda x : x[:4]).astype(int)\n",
    "df_users[\"victim_age\"] = df_users[\"year_acc\"]-df_users[\"an_nais\"]\n",
    "for i in df_users[\"victim_age\"] :\n",
    "        if (i>120)|(i<0):\n",
    "                df_users[\"victim_age\"].replace(i,np.nan)\n",
    "df_caract[\"hour\"] = df_caract[\"hrmn\"].astype(str).apply(lambda x : x[:-3])\n",
    "df_caract.drop(['hrmn', 'an'], inplace=True, axis=1)\n",
    "df_users.drop(['an_nais'], inplace=True, axis=1)\n",
    "\n",
    "#--Replacing names \n",
    "df_users.grav.replace([1,2,3,4], [1,3,4,2], inplace = True)\n",
    "df_caract.rename({\"agg\" : \"agg_\"},  inplace = True, axis = 1)\n",
    "corse_replace = {\"2A\":\"201\", \"2B\":\"202\"}\n",
    "df_caract[\"dep\"] = df_caract[\"dep\"].str.replace(\"2A\", \"201\")\n",
    "df_caract[\"dep\"] = df_caract[\"dep\"].str.replace(\"2B\", \"202\")\n",
    "df_caract[\"com\"] = df_caract[\"com\"].str.replace(\"2A\", \"201\")\n",
    "df_caract[\"com\"] = df_caract[\"com\"].str.replace(\"2B\", \"202\")\n",
    "\n",
    "#--Converting columns types\n",
    "df_caract[[\"dep\",\"com\", \"hour\"]] = df_caract[[\"dep\",\"com\", \"hour\"]].astype(int)\n",
    "\n",
    "dico_to_float = { 'lat': float, 'long':float}\n",
    "df_caract[\"lat\"] = df_caract[\"lat\"].str.replace(',', '.')\n",
    "df_caract[\"long\"] = df_caract[\"long\"].str.replace(',', '.')\n",
    "df_caract = df_caract.astype(dico_to_float)\n",
    "\n",
    "\n",
    "#--Grouping modalities \n",
    "dico = {1:0, 2:1, 3:1, 4:1, 5:1, 6:1,7:1, 8:0, 9:0}\n",
    "df_caract[\"atm\"] = df_caract[\"atm\"].replace(dico)\n",
    "catv_value = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,30,31,32,33,34,35,36,37,38,39,40,41,42,43,50,60,80,99]\n",
    "catv_value_new = [0,1,1,2,1,1,6,2,5,5,5,5,5,4,4,4,4,4,3,3,4,4,1,1,1,1,1,6,6,3,3,3,3,1,1,1,1,1,0,0]\n",
    "df_veh['catv'].replace(catv_value, catv_value_new, inplace = True)\n",
    "\n",
    "#--Merging datasets \n",
    "fusion1= df_users.merge(df_veh, on = [\"Num_Acc\",\"num_veh\", \"id_vehicule\"], how=\"inner\")\n",
    "fusion1 = fusion1.sort_values(by = \"grav\", ascending = False)\n",
    "fusion1 = fusion1.drop_duplicates(subset = ['Num_Acc'], keep=\"first\")\n",
    "fusion2 = fusion1.merge(df_places, on = \"Num_Acc\", how = \"left\")\n",
    "df = fusion2.merge(df_caract, on = 'Num_Acc', how=\"left\")\n",
    "\n",
    "#--Adding new columns\n",
    "df = df.merge(nb_victim, on = \"Num_Acc\", how = \"inner\")\n",
    "df.rename({\"count\" :\"nb_victim\"},axis = 1, inplace = True) \n",
    "df = df.merge(nb_vehicules, on = \"Num_Acc\", how = \"inner\") \n",
    "df.rename({\"count\" :\"nb_vehicules\"},axis = 1, inplace = True)\n",
    "\n",
    "#--Modification of the target variable  : 1 : prioritary // 0 : non-prioritary\n",
    "df['grav'].replace([2,3,4], [0,1,1], inplace=True)\n",
    "\n",
    "\n",
    "#--Replacing values -1 and 0 \n",
    "col_to_replace0_na = [ \"trajet\", \"catv\", \"motor\"]\n",
    "col_to_replace1_na = [ \"trajet\", \"secu1\", \"catv\", \"obsm\", \"motor\", \"circ\", \"surf\", \"situ\", \"vma\", \"atm\", \"col\"]\n",
    "df[col_to_replace1_na] = df[col_to_replace1_na].replace(-1, np.nan)\n",
    "df[col_to_replace0_na] = df[col_to_replace0_na].replace(0, np.nan)\n",
    "\n",
    "\n",
    "#--Dropping columns \n",
    "list_to_drop = ['senc','larrout','actp', 'manv', 'choc', 'nbv', 'prof', 'plan', 'Num_Acc', 'id_vehicule', 'num_veh', 'pr', 'pr1','voie', 'trajet',\"secu2\", \"secu3\",'adr', 'v1', 'lartpc','occutc','v2','vosp','locp','etatp', 'infra', 'obs' ]\n",
    "df.drop(list_to_drop, axis=1, inplace=True)\n",
    "\n",
    "#--Dropping lines with NaN values\n",
    "col_to_drop_lines = ['catv', 'vma', 'secu1', 'obsm', 'atm']\n",
    "df = df.dropna(subset = col_to_drop_lines, axis=0)\n",
    "\n",
    "#--Filling NaN values\n",
    "col_to_fill_na = [\"surf\", \"circ\", \"col\", \"motor\"]\n",
    "df[col_to_fill_na] = df[col_to_fill_na].fillna(df[col_to_fill_na].mode().iloc[0])\n",
    "\n",
    "target = df['grav']\n",
    "feats = df.drop(['grav'], axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feats, target, test_size=0.3, random_state = 42)\n",
    "\n",
    "#--Filling NaN values\n",
    "# col_to_fill_na = [\"surf\", \"circ\", \"col\", \"motor\"]\n",
    "# X_train[col_to_fill_na] = X_train[col_to_fill_na].fillna(X_train[col_to_fill_na].mode().iloc[0])\n",
    "# X_test[col_to_fill_na] = X_test[col_to_fill_na].fillna(X_train[col_to_fill_na].mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select lines to run a gridsearch \n",
    "num_random_rows = 5000\n",
    "X_train_reduced = X_train.sample(n=num_random_rows, random_state = 42)\n",
    "X_train_reduced.shape\n",
    "y_train_reduced = y_train.sample(n=num_random_rows, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best features for the median for categorical variables\n",
    "\n",
    "selector_median = SelectFromModel(SGDClassifier(random_state = 0), threshold = 'median')\n",
    "\n",
    "selector_median.fit_transform(X_train, y_train)\n",
    "\n",
    "feats_sgdc_med = feats.columns[selector_median.get_support()]\n",
    "print(\"les va les plus explicatives selectionnées par mediane sont:\", feats.columns[selector_median.get_support()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_select = LogisticRegression(class_weight='balanced')\n",
    "lr_select.fit(X_train_reduced, y_train_reduced)\n",
    "\n",
    "# Visualization of false positives and false negatives\n",
    "\n",
    "y_pred_lr = lr_select.predict(X_test)\n",
    "\n",
    "print(pd.crosstab(y_test, y_pred_lr, rownames=['Classe réelle'], colnames=['Classe prédite']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_value = f1_score(y_test, y_pred_lr)\n",
    "\n",
    "print(\"F1-score:\", f1_score_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier(criterion = 'entropy', max_depth=4, random_state=123)\n",
    "\n",
    "dt_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prediction of test features and creation of the confusion matrix\n",
    "y_pred = dt_clf.predict(X_test)\n",
    "pd.crosstab(y_test, y_pred, rownames=['Real class'], colnames=['Predict class'])\n",
    "f1_score_value = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"F1-score:\", f1_score_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = {}\n",
    "for feature, importance in zip(df.columns, dt_clf.feature_importances_):\n",
    "    feats[feature] = importance \n",
    "    \n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Importance'})\n",
    "importances.sort_values(by='Importance', ascending=False).head(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=8, random_state=321)\n",
    "dt_clf_gini.fit(X_train, y_train)\n",
    "y_pred = dt_clf_gini.predict(X_test)\n",
    "pd.crosstab(y_test, y_pred, rownames=['Real class'], colnames=['Predict class'])\n",
    "f1_score_value = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"F1-score:\", f1_score_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 7, metric = 'minkowski')\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_minko = []\n",
    "score_man = []\n",
    "score_cheb = []\n",
    "\n",
    "for i in range(1, 10):\n",
    "    knn_mink = neighbors.KNeighborsClassifier(n_neighbors = i, metric = 'minkowski').fit(X_train_reduced.values, y_train_reduced.values)\n",
    "    knn_man = neighbors.KNeighborsClassifier(n_neighbors = i, metric = 'manhattan').fit(X_train_reduced.values, y_train_reduced.values)\n",
    "    knn_cheb = neighbors.KNeighborsClassifier(n_neighbors = i, metric = 'chebyshev').fit(X_train_reduced.values, y_train_reduced.values)\n",
    "    y_pred_mink = knn_mink.predict(X_test.values)\n",
    "    y_pred_man = knn_man.predict(X_test.values)\n",
    "    y_pred_cheb = knn_cheb.predict(X_test.values)\n",
    "    score_minko.append(f1_score(y_test, y_pred_mink))\n",
    "    score_man.append(f1_score(y_test, y_pred_man))\n",
    "    score_cheb.append(f1_score(y_test, y_pred_cheb))\n",
    "\n",
    "print(score_minko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = ensemble.RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "#--Train the model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "\n",
    "X_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ensemble.RandomForestClassifier()\n",
    "\n",
    "# Définir la grille des hyperparamètres à explorer\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Créer l'objet GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Effectuer la recherche sur la grille\n",
    "grid_search.fit(X_train_reduced, y_train_reduced)\n",
    "\n",
    "# Afficher les meilleurs paramètres et score\n",
    "print(\"Meilleurs paramètres:\", grid_search.best_params_)\n",
    "print(\"Meilleur score F1:\", grid_search.best_score_)\n",
    "\n",
    "# Utiliser le modèle avec les meilleurs paramètres\n",
    "best_clf = grid_search.best_estimator_\n",
    "y_pred_rf = best_clf.predict(X_test)\n",
    "\n",
    "# Calculer et afficher le score F1 sur les données de test\n",
    "f1 = f1_score(y_test, y_pred_rf)\n",
    "print(\"Score F1 sur les données de test:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico = {\"place\": 0,\n",
    "\"catu\": 0,\n",
    "\"sexe\" : 0,\n",
    "\"secu1\" : 0.0,\n",
    "\"year_acc\" : 0,\n",
    "\"victim_age\" : 0,\n",
    "\"catv\" : 0,\n",
    "\"obsm\" : 0,\n",
    "\"motor\" : 0,\n",
    "\"catr\" : 0,\n",
    "\"circ\" : 0,\n",
    "\"surf\" : 0,\n",
    "\"situ\" : 0,\n",
    "\"vma\" : 0,\n",
    "\"jour\" :0,\n",
    "\"mois\" : 0,\n",
    "\"lum\" : 0,\n",
    "\"dep\" : 0,\n",
    "\"com\" : 0,\n",
    "\"agg_\" : 0,\n",
    "\"int\" : 0,\n",
    "\"atm\" : 0,\n",
    "\"col\" :0, \n",
    "\"lat\" : 0,\n",
    "\"long\" : 0,\n",
    "\"hour\" : 0,\n",
    "\"nb_victim\" : 0,\n",
    "\"nb_vehicules\" : 0}\n",
    "\n",
    "df_test = pd.DataFrame([dico])\n",
    "print(rf_classifier.predict(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimez la limite de lignes et de colonnes pour l'affichage\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "y_pred_rf\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "\n",
    "rOs = RandomOverSampler()\n",
    "X_ro, y_ro = rOs.fit_resample(X_train, y_train)\n",
    "print('Classes échantillon oversampled :', dict(pd.Series(y_ro).value_counts()))\n",
    "\n",
    "#SMOTE\n",
    "smo = SMOTE()\n",
    "X_sm, y_sm = smo.fit_resample(X_train, y_train)\n",
    "print('Classes échantillon SMOTE :', dict(pd.Series(y_sm).value_counts()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
